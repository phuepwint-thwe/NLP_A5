{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Direct Preference Optimization: Your Language Model is Secretly a Reward Model (DPO)](https://arxiv.org/pdf/2305.18290.pdf)\n",
    "\n",
    "### Reference Code \n",
    "- https://huggingface.co/docs/trl/main/en/dpo_trainer\n",
    "- https://github.com/huggingface/trl/blob/main/examples/scripts/dpo.py"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore the final dataset object should contain these 3 entries if you use the default DPODataCollatorWithPadding data collator. \n",
    "\n",
    "The entries should be named:\n",
    "- prompt\n",
    "- chosen\n",
    "- rejected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "# Set GPU device\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n",
    "\n",
    "os.environ['http_proxy']  = 'http://192.41.170.23:3128'\n",
    "os.environ['https_proxy'] = 'http://192.41.170.23:3128'\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install trl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dpo_dataset_dict = {\n",
    "    \"prompt\": [\n",
    "        \"hello\",\n",
    "        \"how are you\",\n",
    "        \"What is your name?\",\n",
    "        \"What is your name?\",\n",
    "        \"Which is the best programming language?\",\n",
    "        \"Which is the best programming language?\",\n",
    "        \"Which is the best programming language?\",\n",
    "    ],\n",
    "    \"chosen\": [\n",
    "        \"hi nice to meet you\",\n",
    "        \"I am fine\",\n",
    "        \"My name is Mary\",\n",
    "        \"My name is Mary\",\n",
    "        \"Python\",\n",
    "        \"Python\",\n",
    "        \"Java\",\n",
    "    ],\n",
    "    \"rejected\": [\n",
    "        \"leave me alone\",\n",
    "        \"I am not fine\",\n",
    "        \"Whats it to you?\",\n",
    "        \"I dont have a name\",\n",
    "        \"Javascript\",\n",
    "        \"C++\",\n",
    "        \"C++\",\n",
    "    ],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import Dataset, load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM, \n",
    "    AutoTokenizer, \n",
    "    HfArgumentParser, \n",
    "    TrainingArguments\n",
    ")\n",
    "\n",
    "from typing import Dict, Optional\n",
    "from trl import DPOTrainer, DPOConfig\n",
    "from huggingface_hub import login"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. load a pretrained model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "model_name_or_path = \"gpt2\"\n",
    "ignore_bias_buffers = False\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name_or_path)\n",
    "# if ignore_bias_buffers:\n",
    "#     # torch distributed hack\n",
    "#     model._ddp_params_and_buffers_to_ignore = [\n",
    "#         name for name, buffer in model.named_buffers() if buffer.dtype == torch.bool\n",
    "#     ]\n",
    "\n",
    "model_ref = AutoModelForCausalLM.from_pretrained(model_name_or_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DPO trainer expects a model of AutoModelForCausalLM, compared to PPO that expects AutoModelForCausalLMWithValueHead for the value function."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load the Dahoas dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract prompt from response\n",
    "def extract_anthropic_prompt(prompt_and_response: str) -> str:\n",
    "    search_term = \"\\n\\nAssistant:\"\n",
    "    search_term_idx = prompt_and_response.rfind(search_term)\n",
    "    assert search_term_idx != -1, f\"Prompt and response does not contain '{search_term}'\"\n",
    "    return prompt_and_response[: search_term_idx + len(search_term)]\n",
    "\n",
    "# Load dataset\n",
    "def get_static_hh(split: str, sanity_check: bool = False, cache_dir: str = None):\n",
    "    dataset = load_dataset(\"Dahoas/static-hh\", split=split, cache_dir=cache_dir)\n",
    "    if sanity_check:\n",
    "        dataset = dataset.select(range(min(len(dataset), 5)))  # Use a smaller dataset for testing\n",
    "\n",
    "    def filter_columns(sample):\n",
    "        return {\n",
    "            \"prompt\": sample[\"prompt\"],\n",
    "            \"chosen\": sample[\"chosen\"],\n",
    "            \"rejected\": sample[\"rejected\"],\n",
    "        }\n",
    "\n",
    "    return dataset.map(filter_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sanity_check = True\n",
    "train_dataset = get_static_hh(\"train\", sanity_check=sanity_check)\n",
    "eval_dataset = get_static_hh(\"test\", sanity_check=sanity_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['prompt', 'response', 'chosen', 'rejected'],\n",
       "    num_rows: 5\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['prompt', 'response', 'chosen', 'rejected'],\n",
       "    num_rows: 5\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. initialize training arguments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning_rate = 1e-3\n",
    "# per_device_train_batch_size = 8\n",
    "# gradient_accumulation_steps = 1\n",
    "# max_length= 512 \n",
    "# max_prompt_length = 128 \n",
    "# max_target_length =128 \n",
    "# label_pad_token_id = 100\n",
    "# max_steps = 1000\n",
    "# # instrumentation\n",
    "# sanity_check = True\n",
    "# report_to = None\n",
    "# gradient_checkpointing = None\n",
    "# beta = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install --upgrade transformers[torch] accelerate>=0.26.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_args = TrainingArguments(\n",
    "#     per_device_train_batch_size=per_device_train_batch_size,\n",
    "#     max_steps=max_steps,\n",
    "#     remove_unused_columns=False,\n",
    "#     gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "#     learning_rate=learning_rate,\n",
    "#     evaluation_strategy=\"steps\",\n",
    "#     logging_first_step=True,\n",
    "#     logging_steps=5,  # match results in blog post\n",
    "#     eval_steps=500,\n",
    "#     output_dir=\"./test\",\n",
    "#     optim=\"rmsprop\",\n",
    "#     warmup_steps=150,\n",
    "#     report_to=report_to,\n",
    "#     bf16=True,\n",
    "#     # gradient_checkpointing=gradient_checkpointing,\n",
    "#     gradient_checkpointing=True,\n",
    "#     # TODO: uncomment that on the next transformers release\n",
    "#     # gradient_checkpointing_kwargs=gradient_checkpointing_kwargs,\n",
    "# )\n",
    "\n",
    "# from trl import DPOConfig\n",
    "\n",
    "# training_args = DPOConfig(\n",
    "#     # beta=0.1,  # Keep this if needed\n",
    "#     output_dir=\"./test\",\n",
    "#     per_device_train_batch_size=8,\n",
    "#     gradient_accumulation_steps=1,\n",
    "#     learning_rate=1e-3,\n",
    "#     evaluation_strategy=\"steps\",\n",
    "#     logging_steps=5,\n",
    "#     eval_steps=500,\n",
    "#     max_steps=1000,\n",
    "#     optim=\"rmsprop\",\n",
    "#     warmup_steps=150,\n",
    "#     bf16=True,\n",
    "#     gradient_checkpointing=True,\n",
    "# )\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. initialize the DPO trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dpo_trainer = DPOTrainer(\n",
    "#     model,\n",
    "#     model_ref,\n",
    "#     args=training_args,\n",
    "#     beta=beta,\n",
    "#     train_dataset=train_dataset,\n",
    "#     eval_dataset=eval_dataset,\n",
    "#     tokenizer=tokenizer,\n",
    "#     max_length=max_length,\n",
    "#     max_target_length=max_target_length,\n",
    "#     max_prompt_length=max_prompt_length,\n",
    "#     generate_during_eval=True,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from trl import DPOTrainer\n",
    "\n",
    "# dpo_trainer = DPOTrainer(\n",
    "#     model=model,\n",
    "#     ref_model=model_ref,  # Ensure model_ref is correctly defined\n",
    "#     args=training_args,\n",
    "#     train_dataset=train_dataset,\n",
    "#     eval_dataset=eval_dataset,\n",
    "#     tokenizer=tokenizer,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import trl\n",
    "# print(trl.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install --upgrade trl"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dpo_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Training with lr=0.001, batch_size=8, epochs=5, beta=0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-st124784/.local/lib/python3.12/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_3756928/361167526.py:45: FutureWarning: `tokenizer` is deprecated and removed starting from version 0.16.0 for `DPOTrainer.__init__`. Use `processing_class` instead.\n",
      "  dpo_trainer = DPOTrainer(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3a8d2cc5811420d9e6f05de5c098089",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying chat template to train dataset:   0%|          | 0/5 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75b69a0a76944b7588ccba93e9dfc182",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/5 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38c1e2c4cf7c4a4595d643e2d186ab4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying chat template to eval dataset:   0%|          | 0/5 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fae37e7225f449708e583fa41407b560",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing eval dataset:   0%|          | 0/5 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5/5 00:36, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rewards/chosen</th>\n",
       "      <th>Rewards/rejected</th>\n",
       "      <th>Rewards/accuracies</th>\n",
       "      <th>Rewards/margins</th>\n",
       "      <th>Logps/chosen</th>\n",
       "      <th>Logps/rejected</th>\n",
       "      <th>Logits/chosen</th>\n",
       "      <th>Logits/rejected</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-267.235687</td>\n",
       "      <td>-185.095245</td>\n",
       "      <td>-108.793655</td>\n",
       "      <td>-151.277023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.620372</td>\n",
       "      <td>-0.078956</td>\n",
       "      <td>-0.251735</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.172779</td>\n",
       "      <td>-268.025238</td>\n",
       "      <td>-187.612595</td>\n",
       "      <td>-108.781120</td>\n",
       "      <td>-151.336227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.636784</td>\n",
       "      <td>0.063790</td>\n",
       "      <td>-0.054355</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.118144</td>\n",
       "      <td>-266.597778</td>\n",
       "      <td>-185.638779</td>\n",
       "      <td>-108.687149</td>\n",
       "      <td>-151.747314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.595837</td>\n",
       "      <td>-0.017816</td>\n",
       "      <td>-0.237263</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.219446</td>\n",
       "      <td>-267.413818</td>\n",
       "      <td>-187.467865</td>\n",
       "      <td>-109.276733</td>\n",
       "      <td>-152.481323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.494677</td>\n",
       "      <td>0.077860</td>\n",
       "      <td>-0.382726</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.460586</td>\n",
       "      <td>-266.457062</td>\n",
       "      <td>-188.922485</td>\n",
       "      <td>-109.685966</td>\n",
       "      <td>-153.107025</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 : < :]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " New best model found! Saving at: ./dpo_lr0.001_bs8_ep5_beta0.1\n",
      "\n",
      " Experiment Results:\n",
      "{'learning_rate': 0.001, 'batch_size': 8, 'epochs': 5, 'beta': 0.1, 'loss': 0.4946766495704651}\n",
      "\n",
      " Best model saved at: ./dpo_lr0.001_bs8_ep5_beta0.1\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import torch\n",
    "from trl import DPOConfig, DPOTrainer\n",
    "\n",
    "# Define hyperparameter search space\n",
    "learning_rates = [1e-3]\n",
    "batch_sizes = [8]\n",
    "num_epochs = [5]\n",
    "betas = [0.1]\n",
    "\n",
    "# Generate all hyperparameter combinations\n",
    "hyperparameter_combinations = list(itertools.product(learning_rates, batch_sizes, num_epochs, betas))\n",
    "\n",
    "# Track best model\n",
    "results = []\n",
    "best_loss = float(\"inf\")  \n",
    "best_model_path = None\n",
    "\n",
    "# Loop through hyperparameter combinations\n",
    "for lr, batch_size, epochs, beta in hyperparameter_combinations:\n",
    "    print(f\"\\n Training with lr={lr}, batch_size={batch_size}, epochs={epochs}, beta={beta}\")\n",
    "    \n",
    "    output_dir = f\"./dpo_lr{lr}_bs{batch_size}_ep{epochs}_beta{beta}\"\n",
    "\n",
    "    # Define DPO configuration\n",
    "    dpo_config = DPOConfig(\n",
    "        output_dir=output_dir,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        num_train_epochs=epochs,\n",
    "        gradient_accumulation_steps=1,\n",
    "        learning_rate=lr,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        logging_dir=\"./logs\",\n",
    "        logging_steps=10,\n",
    "        save_total_limit=2,\n",
    "        warmup_steps=150,\n",
    "        bf16=torch.cuda.is_available(),  # Use bf16 if GPU supports it\n",
    "        gradient_checkpointing=True,\n",
    "        report_to=\"none\",\n",
    "    )\n",
    "\n",
    "    # Initialize trainer\n",
    "    dpo_trainer = DPOTrainer(\n",
    "        model=model,\n",
    "        ref_model=model_ref,\n",
    "        args=dpo_config,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "\n",
    "    # Train model\n",
    "    dpo_trainer.train()\n",
    "\n",
    "    # Evaluate model\n",
    "    eval_results = dpo_trainer.evaluate()\n",
    "    loss = eval_results.get(\"eval_loss\", None)\n",
    "    \n",
    "    # Store results\n",
    "    results.append({\n",
    "        \"learning_rate\": lr,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"epochs\": epochs,\n",
    "        \"beta\": beta,\n",
    "        \"loss\": loss\n",
    "    })\n",
    "\n",
    "    # Save the best model based on eval loss\n",
    "    if loss is not None and loss < best_loss:\n",
    "        best_loss = loss\n",
    "        best_model_path = output_dir\n",
    "        print(f\" New best model found! Saving at: {best_model_path}\")\n",
    "\n",
    "# Print results\n",
    "print(\"\\n Experiment Results:\")\n",
    "for res in results:\n",
    "    print(res)\n",
    "\n",
    "# Save best model path\n",
    "if best_model_path:\n",
    "    print(f\"\\n Best model saved at: {best_model_path}\")\n",
    "else:\n",
    "    print(\"\\n No model improved from the initial loss.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Load model and tokenizer from saved directory\n",
    "save_directory = \"./dpo_lr0.001_bs8_ep5_beta0.1/checkpoint-4\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(save_directory).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(save_directory)\n",
    "\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: What is love?\n",
      "Response: Love. It's a word that comes from the Greek meaning \"love\" and it means to be loved, not just for yourself but also because of your own actions or feelings about others (e-mail). The term was first used in 1876\n"
     ]
    }
   ],
   "source": [
    "# Ensure tokenizer padding token is set correctly\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token  # Assign pad token\n",
    "\n",
    "# Move model to the correct device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "def generate_response(prompt, max_tokens=100):\n",
    "    try:\n",
    "        # Format input as a dialogue\n",
    "        formatted_prompt = f\"Human: {prompt}\\n\\nAssistant:\"\n",
    "\n",
    "        # Tokenize input and move to the correct device\n",
    "        input_ids = tokenizer(\n",
    "            formatted_prompt, return_tensors=\"pt\", padding=True, truncation=True\n",
    "        ).input_ids.to(device)\n",
    "\n",
    "        # Generate response with optimized settings\n",
    "        with torch.no_grad():\n",
    "            output_ids = model.generate(\n",
    "                input_ids,\n",
    "                max_new_tokens=50,  # Response length\n",
    "                temperature=0.4,  # Lower for more focused responses\n",
    "                top_p=0.8,  # Nucleus sampling\n",
    "                top_k=30,  # Limits token selection to top 40 words\n",
    "                repetition_penalty=1.5,  # Stronger penalty against repetition\n",
    "                do_sample=False,  # Enables diverse responses\n",
    "                pad_token_id=tokenizer.pad_token_id,  # Uses tokenizer's pad token\n",
    "            )\n",
    "\n",
    "        # Decode and clean response\n",
    "        full_response = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "        response = full_response.split(\"\\n\\nAssistant:\")[-1].strip()  # Remove redundant parts\n",
    "\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        return f\"Error generating response: {str(e)}\"\n",
    "\n",
    "# Example test case\n",
    "sample_prompt = \"What is love?\"\n",
    "\n",
    "# Generate and print the response\n",
    "response = generate_response(sample_prompt)\n",
    "print(f\"Prompt: {sample_prompt}\\nResponse: {response}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Who is the president of USA?\n",
      "Response: Q: Who is the president of USA?\n",
      "A: The United States President. He's a guy who has been around for over 20 years, and he knows how to run an organization that runs like it does business in America â€“ which makes him very important because you have people from all walks of life coming up here at this time when there are so many different types of businesses operating out on our soilâ€¦I think we're going through some pretty big changes right now with regard not just health care but also education as well.\"\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Ensure the tokenizer has a pad token\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "def generate_response(prompt, max_tokens=100):\n",
    "    try:\n",
    "        # âœ… Provide a more explicit instruction\n",
    "        formatted_prompt = f\"Q: {prompt}\\nA:\"\n",
    "\n",
    "        # Tokenize input\n",
    "        input_ids = tokenizer(\n",
    "            formatted_prompt, return_tensors=\"pt\", padding=True, truncation=True\n",
    "        ).input_ids.to(device)\n",
    "\n",
    "        # Generate response with improved settings\n",
    "        with torch.no_grad():\n",
    "            output_ids = model.generate(\n",
    "                input_ids,\n",
    "                max_new_tokens=max_tokens,\n",
    "                temperature=0.5,  # Increase randomness\n",
    "                top_p=0.85,  # Balance nucleus sampling\n",
    "                top_k=50,  # Increase diversity\n",
    "                repetition_penalty=1.4,  # Reduce repetitive responses\n",
    "                do_sample=True,  # Enable sampling for variability\n",
    "                eos_token_id=tokenizer.eos_token_id,  # Ensure stopping criteria\n",
    "                pad_token_id=tokenizer.pad_token_id,  # Avoid padding issues\n",
    "            )\n",
    "\n",
    "        # Decode and clean response\n",
    "        response = tokenizer.decode(output_ids[0], skip_special_tokens=True).strip()\n",
    "\n",
    "        # âœ… Ensure we don't return the prompt itself\n",
    "        if response.lower() == prompt.lower():\n",
    "            return \"I am not sure about that, but I can try to help with more information!\"\n",
    "\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        return f\"Error generating response: {str(e)}\"\n",
    "\n",
    "# Example test case\n",
    "sample_prompt = \"Who is the president of USA?\"\n",
    "response = generate_response(sample_prompt)\n",
    "print(f\"Prompt: {sample_prompt}\\nResponse: {response}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
